{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary depencencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import text_normalizer as tn\n",
    "import model_evaluation_utils as meu  ## Not provided to you\n",
    "# The Intern can code the methods to print the Metric like accuracy,\n",
    "# and confusion matrix. The o/ps in the below cells would give an idea.\n",
    "\n",
    "np.set_printoptions(precision=2, linewidth=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "completed\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(r'IMDB Dataset.csv')\n",
    "\n",
    "# take a peek at the data\n",
    "print(dataset.head())\n",
    "reviews = np.array(dataset['review'])\n",
    "sentiments = np.array(dataset['sentiment'])\n",
    "\n",
    "# build train and test datasets\n",
    "train_reviews = reviews[:35000]\n",
    "train_sentiments = sentiments[:35000]\n",
    "test_reviews = reviews[35000:]\n",
    "test_sentiments = sentiments[35000:]\n",
    "print(\"completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# normalize datasets\n",
    "norm_train_reviews = tn.normalize_corpus(train_reviews)\n",
    "norm_test_reviews = tn.normalize_corpus(test_reviews)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional Supervised Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# build BOW features on train reviews\n",
    "cv = CountVectorizer(binary=False, min_df=0.0, max_df=1.0, ngram_range=(1,2))\n",
    "cv_train_features = cv.fit_transform(norm_train_reviews)\n",
    "\n",
    "# build TFIDF features on train reviews\n",
    "tv = TfidfVectorizer(use_idf=True, min_df=0.0, max_df=1.0, ngram_range=(1,2),\n",
    "                     sublinear_tf=True)\n",
    "tv_train_features = tv.fit_transform(norm_train_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform test reviews into features\n",
    "cv_test_features = cv.transform(norm_test_reviews)\n",
    "tv_test_features = tv.transform(norm_test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW model:> Train features shape: (35000, 2116029)  Test features shape: (15000, 2116029)\n",
      "TFIDF model:> Train features shape: (35000, 2116029)  Test features shape: (15000, 2116029)\n"
     ]
    }
   ],
   "source": [
    "print('BOW model:> Train features shape:', cv_train_features.shape, ' Test features shape:', cv_test_features.shape)\n",
    "print('TFIDF model:> Train features shape:', tv_train_features.shape, ' Test features shape:', tv_test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training, Prediction and Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(penalty='l2', max_iter=100, C=1)\n",
    "svm = SGDClassifier(loss='hinge', max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaypatel/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.9059\n",
      "Precision: 0.9059\n",
      "Recall: 0.9059\n",
      "F1 Score: 0.9059\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.90      0.91      0.91      7510\n",
      "    negative       0.91      0.90      0.91      7490\n",
      "\n",
      "    accuracy                           0.91     15000\n",
      "   macro avg       0.91      0.91      0.91     15000\n",
      "weighted avg       0.91      0.91      0.91     15000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__new__() got an unexpected keyword argument 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-fdb224a2f18b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                              \u001b[0mtrain_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv_train_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_sentiments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                              test_features=cv_test_features, test_labels=test_sentiments)\n\u001b[0;32m----> 6\u001b[0;31m meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=lr_bow_predictions,\n\u001b[0m\u001b[1;32m      7\u001b[0m                                       classes=['positive', 'negative'])\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/Sentiment Analysis Project Specs n Resources/model_evaluation_utils.py\u001b[0m in \u001b[0;36mdisplay_model_performance_metrics\u001b[0;34m(true_labels, predicted_labels, classes)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nPrediction Confusion Matrix:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     display_confusion_matrix(true_labels=true_labels, predicted_labels=predicted_labels, \n\u001b[0m\u001b[1;32m     86\u001b[0m                              classes=classes)\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/Sentiment Analysis Project Specs n Resources/model_evaluation_utils.py\u001b[0m in \u001b[0;36mdisplay_confusion_matrix\u001b[0;34m(true_labels, predicted_labels, classes)\u001b[0m\n\u001b[1;32m     58\u001b[0m                                   labels=classes)\n\u001b[1;32m     59\u001b[0m     cm_frame = pd.DataFrame(data=cm, \n\u001b[0;32m---> 60\u001b[0;31m                             columns=pd.MultiIndex(levels=[['Predicted:'], classes], \n\u001b[0m\u001b[1;32m     61\u001b[0m                                                   labels=level_labels), \n\u001b[1;32m     62\u001b[0m                             index=pd.MultiIndex(levels=[['Actual:'], classes], \n",
      "\u001b[0;31mTypeError\u001b[0m: __new__() got an unexpected keyword argument 'labels'"
     ]
    }
   ],
   "source": [
    "# Logistic Regression model on BOW features\n",
    "# Please Note : the module meu is not been provided. \n",
    "lr_bow_predictions = meu.train_predict_model(classifier=lr, \n",
    "                                             train_features=cv_train_features, train_labels=train_sentiments,\n",
    "                                             test_features=cv_test_features, test_labels=test_sentiments)\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=lr_bow_predictions,\n",
    "                                      classes=['positive', 'negative'])\n",
    "\n",
    "# THE BELOW O/P SHOULD GIVE YOU A FAIR IDEA ON WHAT :\n",
    "# methods like \n",
    "# train_predict_model() are doing and printing as o/p.\n",
    "# display_model_performance_metrics() are doing and printing as o/p.\n",
    "\n",
    "# As an Intern you are not suppose to produce the exact o/p \n",
    "# You may only code the minimum required metrics which helps you to \n",
    "# compare the different ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.8941\n",
      "Precision: 0.8942\n",
      "Recall: 0.8941\n",
      "F1 Score: 0.8941\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.89      0.90      0.90      7510\n",
      "    negative       0.90      0.89      0.89      7490\n",
      "\n",
      "    accuracy                           0.89     15000\n",
      "   macro avg       0.89      0.89      0.89     15000\n",
      "weighted avg       0.89      0.89      0.89     15000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__new__() got an unexpected keyword argument 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-091a5d043611>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                \u001b[0mtrain_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtv_train_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_sentiments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                test_features=tv_test_features, test_labels=test_sentiments)\n\u001b[0;32m----> 6\u001b[0;31m meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=lr_tfidf_predictions,\n\u001b[0m\u001b[1;32m      7\u001b[0m                                       classes=['positive', 'negative'])\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/Sentiment Analysis Project Specs n Resources/model_evaluation_utils.py\u001b[0m in \u001b[0;36mdisplay_model_performance_metrics\u001b[0;34m(true_labels, predicted_labels, classes)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nPrediction Confusion Matrix:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     display_confusion_matrix(true_labels=true_labels, predicted_labels=predicted_labels, \n\u001b[0m\u001b[1;32m     86\u001b[0m                              classes=classes)\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/Sentiment Analysis Project Specs n Resources/model_evaluation_utils.py\u001b[0m in \u001b[0;36mdisplay_confusion_matrix\u001b[0;34m(true_labels, predicted_labels, classes)\u001b[0m\n\u001b[1;32m     58\u001b[0m                                   labels=classes)\n\u001b[1;32m     59\u001b[0m     cm_frame = pd.DataFrame(data=cm, \n\u001b[0;32m---> 60\u001b[0;31m                             columns=pd.MultiIndex(levels=[['Predicted:'], classes], \n\u001b[0m\u001b[1;32m     61\u001b[0m                                                   labels=level_labels), \n\u001b[1;32m     62\u001b[0m                             index=pd.MultiIndex(levels=[['Actual:'], classes], \n",
      "\u001b[0;31mTypeError\u001b[0m: __new__() got an unexpected keyword argument 'labels'"
     ]
    }
   ],
   "source": [
    "# Logistic Regression model on TF-IDF features\n",
    "# Please Note : the module meu is not been provided.\n",
    "lr_tfidf_predictions = meu.train_predict_model(classifier=lr, \n",
    "                                               train_features=tv_train_features, train_labels=train_sentiments,\n",
    "                                               test_features=tv_test_features, test_labels=test_sentiments)\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=lr_tfidf_predictions,\n",
    "                                      classes=['positive', 'negative'])\n",
    "\n",
    "# THE BELOW O/P SHOULD GIVE YOU A FAIR IDEA ON WHAT :\n",
    "# methods like \n",
    "# train_predict_model() are doing and printing as o/p.\n",
    "# display_model_performance_metrics() are doing and printing as o/p.\n",
    "\n",
    "# As an Intern you are not suppose to produce the exact o/p \n",
    "# You may only code the minimum required metrics which helps you to \n",
    "# compare the different ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.9004\n",
      "Precision: 0.9005\n",
      "Recall: 0.9004\n",
      "F1 Score: 0.9004\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.89      0.91      0.90      7510\n",
      "    negative       0.91      0.89      0.90      7490\n",
      "\n",
      "    accuracy                           0.90     15000\n",
      "   macro avg       0.90      0.90      0.90     15000\n",
      "weighted avg       0.90      0.90      0.90     15000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__new__() got an unexpected keyword argument 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-35b44736b923>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                              \u001b[0mtrain_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv_train_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_sentiments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                              test_features=cv_test_features, test_labels=test_sentiments)\n\u001b[0;32m----> 6\u001b[0;31m meu.display_model_performance_metrics(true_labels=test_sentiments, \n\u001b[0m\u001b[1;32m      7\u001b[0m                                       \u001b[0mpredicted_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msvm_bow_predictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                       classes=['positive', 'negative'])\n",
      "\u001b[0;32m~/Downloads/Sentiment Analysis Project Specs n Resources/model_evaluation_utils.py\u001b[0m in \u001b[0;36mdisplay_model_performance_metrics\u001b[0;34m(true_labels, predicted_labels, classes)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nPrediction Confusion Matrix:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     display_confusion_matrix(true_labels=true_labels, predicted_labels=predicted_labels, \n\u001b[0m\u001b[1;32m     86\u001b[0m                              classes=classes)\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/Sentiment Analysis Project Specs n Resources/model_evaluation_utils.py\u001b[0m in \u001b[0;36mdisplay_confusion_matrix\u001b[0;34m(true_labels, predicted_labels, classes)\u001b[0m\n\u001b[1;32m     58\u001b[0m                                   labels=classes)\n\u001b[1;32m     59\u001b[0m     cm_frame = pd.DataFrame(data=cm, \n\u001b[0;32m---> 60\u001b[0;31m                             columns=pd.MultiIndex(levels=[['Predicted:'], classes], \n\u001b[0m\u001b[1;32m     61\u001b[0m                                                   labels=level_labels), \n\u001b[1;32m     62\u001b[0m                             index=pd.MultiIndex(levels=[['Actual:'], classes], \n",
      "\u001b[0;31mTypeError\u001b[0m: __new__() got an unexpected keyword argument 'labels'"
     ]
    }
   ],
   "source": [
    "# SVM model on BOW features\n",
    "# Please Note : the module meu is not been provided.\n",
    "svm_bow_predictions = meu.train_predict_model(classifier=svm, \n",
    "                                             train_features=cv_train_features, train_labels=train_sentiments,\n",
    "                                             test_features=cv_test_features, test_labels=test_sentiments)\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, \n",
    "                                      predicted_labels=svm_bow_predictions,\n",
    "                                      classes=['positive', 'negative'])\n",
    "\n",
    "# THE BELOW O/P SHOULD GIVE YOU A FAIR IDEA ON WHAT :\n",
    "# methods like \n",
    "# train_predict_model() are doing and printing as o/p.\n",
    "# display_model_performance_metrics() are doing and printing as o/p.\n",
    "\n",
    "# As an Intern you are not suppose to produce the exact o/p \n",
    "# You may only code the minimum required metrics which helps you to \n",
    "# compare the different ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.8972\n",
      "Precision: 0.8976\n",
      "Recall: 0.8972\n",
      "F1 Score: 0.8972\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.89      0.91      0.90      7510\n",
      "    negative       0.91      0.88      0.90      7490\n",
      "\n",
      "    accuracy                           0.90     15000\n",
      "   macro avg       0.90      0.90      0.90     15000\n",
      "weighted avg       0.90      0.90      0.90     15000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__new__() got an unexpected keyword argument 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-120c7d1cb01b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                 \u001b[0mtrain_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtv_train_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_sentiments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                 test_features=tv_test_features, test_labels=test_sentiments)\n\u001b[0;32m----> 6\u001b[0;31m meu.display_model_performance_metrics(true_labels=test_sentiments, \n\u001b[0m\u001b[1;32m      7\u001b[0m                                       \u001b[0mpredicted_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msvm_tfidf_predictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                       classes=['positive', 'negative'])\n",
      "\u001b[0;32m~/Downloads/Sentiment Analysis Project Specs n Resources/model_evaluation_utils.py\u001b[0m in \u001b[0;36mdisplay_model_performance_metrics\u001b[0;34m(true_labels, predicted_labels, classes)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nPrediction Confusion Matrix:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     display_confusion_matrix(true_labels=true_labels, predicted_labels=predicted_labels, \n\u001b[0m\u001b[1;32m     86\u001b[0m                              classes=classes)\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/Sentiment Analysis Project Specs n Resources/model_evaluation_utils.py\u001b[0m in \u001b[0;36mdisplay_confusion_matrix\u001b[0;34m(true_labels, predicted_labels, classes)\u001b[0m\n\u001b[1;32m     58\u001b[0m                                   labels=classes)\n\u001b[1;32m     59\u001b[0m     cm_frame = pd.DataFrame(data=cm, \n\u001b[0;32m---> 60\u001b[0;31m                             columns=pd.MultiIndex(levels=[['Predicted:'], classes], \n\u001b[0m\u001b[1;32m     61\u001b[0m                                                   labels=level_labels), \n\u001b[1;32m     62\u001b[0m                             index=pd.MultiIndex(levels=[['Actual:'], classes], \n",
      "\u001b[0;31mTypeError\u001b[0m: __new__() got an unexpected keyword argument 'labels'"
     ]
    }
   ],
   "source": [
    "# SVM model on TF-IDF features\n",
    "# Please Note : the module meu is not been provided.\n",
    "svm_tfidf_predictions = meu.train_predict_model(classifier=svm, \n",
    "                                                train_features=tv_train_features, train_labels=train_sentiments,\n",
    "                                                test_features=tv_test_features, test_labels=test_sentiments)\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, \n",
    "                                      predicted_labels=svm_tfidf_predictions,\n",
    "                                      classes=['positive', 'negative'])\n",
    "\n",
    "# THE BELOW O/P SHOULD GIVE YOU A FAIR IDEA ON WHAT :\n",
    "# methods like \n",
    "# train_predict_model() are doing and printing as o/p.\n",
    "# display_model_performance_metrics() are doing and printing as o/p.\n",
    "\n",
    "# As an Intern you are not suppose to produce the exact o/p \n",
    "# You may only code the minimum required metrics which helps you to \n",
    "# compare the different ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
